\section{Threats to Validity}
The main threats to validity for this paper are the amount of different problems and simulations that have been tested, but also the values of the parameters for each Tree Policy and the quality of the source code. 

\subsection{Amount of problems}
We have run experiments for two different types of problems (\rock and \poc) with varying sizes. There are however many different domains that MCTS is applicable on. Each domain has its own characteristics and Tree Policies can have a different performance for each of them as has been shown during the experiments, most notifiable being \eroulette. In \rock this policy was performing rather well with regard to the other.

\subsection{Number of Simulations}
Experiments were performed using a limited number of simulations. The amount so far has not yet been enough to show a limit of performance for each of the Tree Policies. It is therefore unclear how the Tree Policies will behave when the number of simulations will increase. For instance, there could be policies that end up deteriorating in performance when executed with a large amount of simulations, the same behaviour that can be seen when overfitting occurs in reinforcement learning.

\subsection{Parameters for Tree Policies}
For the experiments, parameters needed to be chosen for the Tree policies. Even though there was some reasoning behind these values, there is no guarantee that all of them are as close to their optimal values as they should be. If for instance an algorithm $A$ can achieve a performance of 100 in the optimal case and an algorithm $B$ a performance of 90 and by setting parameters for $A$ and $B$ such that $B$ reaches its optimal performance and $A$ reaches 85, then that does not make $B$ a better algorithm.