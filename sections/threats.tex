\section{Threats to Validity}
This research constitutes an exploratory study of search heuristics, investigating their relative performance on a relatively small number of problems. As such, the main threats to the validity of the research described in this paper are external, reflecting the degree to which results presented here generalize to other datasets. This concerns the number of different problems investigated, the number of simulations used  and but the values of the parameters for each Tree Policy.

\subsection{Number of problems}
We have run experiments for two different types of problems belonging to the POMDP domain (\rock and \poc) with varying sizes. MCTS, however, has found application in many different domains and it is not unlikely that search trees in some other domains have fundamentally different characteristics. This is even reflected in our own experiments, where we found that \eroulette performed substantially better in \rock than in \poc. To reduce this threat, we have chosen a sample of problems and problem sizes that reflects prior work on POMDPs. Furthermore, \rsoft yielded promising results in an unrelated test following communication with E. Walraven. Further investigation of the evaluated heuristics is needed to evaluate these results on different domains. \\ \\

\subsection{Number of Simulations}
Experiments were performed using a limited number of simulations (8 to 512). These numbers have so far not been enough to show a limit on the performance for each of the Tree Policies. It is therefore unclear how the Tree Policies will behave when the number of simulations (greatly) increases. In particular, we expect some policies to end up deteriorating in performance when executed with a large number of simulations, the same behaviour that can be seen when overfitting occurs in reinforcement learning.

\subsection{Parameters for Tree Policies}
\label{subsec:params}
For the experiments, parameters needed to be chosen for the Tree policies. This concerned in particular \egreedy and \soft. For the former, we chose $\varepsilon$ (the probability of sampling an action at random) to be 0.2, after observing that it was fairly robust with respect to variations in this number. In practice, a number of ML strategies may be used to establish the optimal value for this parameter, which may benefit the performance of \egreedy. The \soft algorithm required tuning of both the start and end values of $\tau$ and the rate of decay. We found that exponential decay yielded substantially superior performance to linear decay. For the values of $\tau$, we hand-tuned the algorithm on each problem instance. Different values of $\tau$ may improve the results for different numbers of simulations as well. Given the already excellent performance of \soft, we deem it unlikely that this substantially influences the overall results.