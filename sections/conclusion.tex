\section{conclusion}
In this paper, we have evaluated a number of Tree Policies that are being used by MCTS in order to solve intractable problems. This evaluation took place on the problems of \rock and \poc: two POMDP problems that cover both static and dynamic environments and represent a substantial variety in state-space sizes. By investigating three different heuristics and two newly constructed ones, we derived a number of desirable and undesirable properties for heuristics in MCTS. We furthermore found that there is room for improvement over the traditionally used UCB1 heuristic, most notably from \soft. Finally, we proposed a variation on \soft which requires less tuning while yielding comparable performance.

This exploratory study is far from exhaustive, but hopes to shed some light on the qualities of a `golden' (general-purpose) Tree Policy for MCTS problems. Future work may extend our investigation to problems that are both larger and come from different domains. Obtaining better performance may also be feasible through more investigation of the ideas behind successful Tree Policies. For instance, preliminary results indicate that taking into account the number of times that an action has already been visited, as UCB1 does, may improve the obtained results of the \soft algorithm as well. Another potentially viable option for future work is to investigate whether the algorithm should behave differently depending on how deep it currently is in the simulation tree, as deeper branches are ever less likely to be visited many times.