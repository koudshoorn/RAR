%!TEX root=../report.tex
\section{Introduction}
Many problems are generally intractable due to exponentially large state-spaces. This category of problems includes both classical optimization problems, such as TSP and scheduling problems \cite{browne2012survey}, as well as two-player games such as Go, Chess and 4x4x4 Tic-Tac-Toe \cite{chaslot2010monte, sharma2008knowledge}.
Results from the ML and AI communities indicate that many of these problems, despite having exponential state-spaces, have tractable sample complexity guarantees \cite{sharma2008knowledge}. This is exemplified by the success of AI players in the game Chess (state-space: $2.3 \cdot 10^{49}$ positions) against human opponents, culminating in the defeat of Garry Kasparov by IBM's Deep Blue AI \cite{campbell2002deep}.

Other problems have remained out of the reach of AI approaches for many decades: the two-player game Go is generally considered the last game in which humans consistently outperform computer players, having a state-space of $~2.08 \cdot 10^{170}$ positions on the default board size of 19x19 \cite{tromp2007combinatorics}. However, even on 9x9 boards, where the state-space complexity is lower than that of chess, state-of-the-art AI-approaches failed to outperform human players due to a variety of factors such as a high branching factor \cite{browne2012survey}.

Monte Carlo Tree Search (MCTS) methods were the first to defeat human professionals in Go \cite{coulom2007efficient}, and have come to dominate the Computer Go field \cite{chaslot2010monte}. MCTS (independently proposed in the same year in \cite{coulom2007efficient, kocsis2006bandit, chaslot2006monte}), applies the intuition of Monte Carlo simulations, which work by repeated random sampling, to search trees. Monte Carlo simulations \cite{metropolis1985monte}, in turn, had previously found application in many fields as alternatives to solving mathematically complex problems \cite{liu2008monte}, \eg in evaluating complex integrals with bounded error.

The success of MCTS methods in Go, as well as its general formulation \cite{chaslot2010monte}, have sparked a variety of research into their application to different areas of both optimization problems (with varying succes, \eg \cite{rimmel2011optimization, cazenave2009monte}) and games \cite{schadd2008single, gelly2012grand}.\footnote{See http://www.ualberta.ca/$\sim$szepesva/MCTS/ for a comprehensive overview.} However, in order to be successful, these applications have all but consistently had to replace the (default) UCB1 selection heuristic, which is at the center of the MCTS process. This suggests that the standard approach fails to make optimal use of the structure and information of the search tree.

We propose to investigate a number of alternative tree search heuristics in order to derive properties of the search tree in general and qualities that good heuristics should posses. We evaluate the heuristics on small and large instances of two problems: \rock and \poc (static environment, $10^4 - 10^7$ states \vs dynamic environment, $10^{11} - 10^{18}$ states). These problems fall into the category of games with partially observable state, meaning that in general only a belief about the state of the game is known. We identify four main results:
\vspace{-4mm}
\begin{enumerate}[topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex]
\item A greedy strategy is poorly equipped when many actions can be chosen.
\item Greater state-spaces necessitate increasingly greedy investigation.
\item ~\soft yields superior performance by connecting the degree of exploration to the total number of simulations.
\item ~\rsoft (our novel variation on \soft) sacrifices little performance in favor of broader applicability to \soft by disregarding the values of rewards used in favor of their ordering.
\end{enumerate}