%!TEX root=../report.tex
\section{Introduction}
Many problems are generally intractable due to exponentially large state-spaces. This category of problems includes both classical optimization problems, such as TSP and scheduling problems \cite{browne2012survey}, as well as two-player games such as Go, Chess and 4x4x4 Tic-Tac-Toe \cite{chaslot2010monte, sharma2008knowledge}.
Results from the ML and AI communities indicate that many of these problems, despite having exponential state-spaces, have tractable sample complexity guarantees \cite{sharma2008knowledge}. This is exemplified by the success of AI players in the game Chess (state-space: $2.3 \cdot 10^{49}$ positions) against human opponents, arguably culminating in the defeat of Garry Kasparov by IBM's Deep Blue AI \cite{campbell2002deep}.

Other problems have remained out of the reach of AI approaches for many decades: the two-player game Go is generally considered the last game in which humans consistently outperform computer players, having a state-space of $~2.08 \cdot 10^{170}$ positions on the default board size of 19x19 \cite{tromp2007combinatorics}. However, even on 9x9 boards, where the state-space complexity is lower than that of chess, state-of-the-art AI-approaches failed to outperform human players due to a variety of factors such as deep trees and a high branching factor \cite{browne2012survey}.

Monte Carlo Tree Search (MCTS) methods were the first to defeat human professionals in Go \cite{coulom2007efficient}, and have come to dominate the Computer Go field \cite{chaslot2010monte}. MCTS (independently proposed in the same year in \cite{coulom2007efficient, kocsis2006bandit, chaslot2006monte}), applies the intuition of Monte Carlo simulations, which work by repeated random sampling, to search trees. Monte Carlo simulations \cite{metropolis1985monte}, in turn, had previously found application in many fields as alternatives to solving mathematically complex problems \cite{liu2008monte}, \eg in evaluating complex integrals with bounded error.

The success of MCTS methods in Go, as well as its general formulation \cite{chaslot2010monte}, have sparked a variety of research into their application to different areas of both optimization problems (with varying succes, \eg \cite{rimmel2011optimization, cazenave2009monte}) and games \cite{schadd2008single, gelly2012grand}.\footnote{See http://www.ualberta.ca/$\sim$szepesva/MCTS/ for a comprehensive overview.} However, successful applications have all but consistently had to adapt the UCB1 tree selection policy, which has come to be regarded as standard, to incorporate heuristics based on the various domains, suggesting that the standard search heuristics fail to generalize well. This suggests that the standard approach fails to make optimal use of the search tree in general.

In this work, we investigate a variety of tree selection policies on small and large instances of two problems: \rock and \poc (static environment, $10^4 - 10^7$ states \vs dynamic environment, $10^{11} - 10^{18}$ states). These problems fall into the category of games with partially observable state, meaning that in general only a belief about the state of the game is known. We find that a simple technique such as \egreedy almost consistently outperforms the UCB1 search policy, particularly in the case of many simulations. Furthermore, two algorithms that behave increasingly more greedy as the number of simulations progress, in particular \soft outperform both these techniques. Finally, we propose a variation on \soft, named \rsoft, that avoids hand-tuning the \emph{temperature} parameter while performing comparable result.