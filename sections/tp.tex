\section{Tree Policies} 

\subsection{UCB1}
UCB stands for Upper Confidence Bounds. It is one of the most standard and well known method in MCTS. The method involves taking a decision based on exploration (finding new, possible optimal, paths) and exploitation (traversing already well performing paths). With this in mind, possible actions are evaluated with the following formula:
\begin{equation}
UCT = \bar{X}_j + 2 C_p\sqrt{\frac{2 \ln n}{n_j}}
\end{equation}
Where $j$ is a child node, $\bar{X}_j$ is the estimated value of $j$, $C_p > 0$ a constant, $n$ the amount of visits of the current node and $n_j$ the amount of visits of $j$.

When each value is obtained for all the child nodes, child with the maximum value will be traversed. It can be seen that $C_p$ can be adjusted to lower or increase the amount of exploration used in contrast to the exploitation of the estimated value $\bar{X}_j$.

\subsection{$\mathcal{E}$-Greedy}

Another well known method is the $\epsilon$-Greedy method. Here, the best estimated child is usually visited, but with a probability $\epsilon$ a child is selected at random, uniformly, independent of the $\bar{X}_j$.

\subsection{SoftMax}
A different approach of the $\epsilon$-Greedy method is SoftMax. Instead of constantly using the same strategy, it uses a variable temperature $\tau$ such that at the start of the algorithm, exploration is preferred and towards the end, exploitation is preferred. 

As such, a child $j$ is visited with probability:
\begin{equation}
P(j) = \frac{e^\frac{\bar{X}_j}{\tau}}{\sum_{b=1}^{n} e^\frac{\bar{X}_j}{\tau}}
\end{equation}
When $\tau$ is large, $P(j)$ approaches a uniform distribution that is independent on $\bar{X}_j$. When $\tau$ approaches zero, SoftMax becomes more greedy. In this sense, SoftMax is quite similar to the local searcher Simulated Annealing.